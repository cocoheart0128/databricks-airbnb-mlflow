{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9ebea79-6135-4391-9246-7cfcec34dbca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": ""
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": {
        "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
        "text/plain": ""
       },
       "datasetInfos": [],
       "executionCount": null,
       "metadata": {
        "kernelSessionId": "fec4a27f-4e1d14398f55a64dad107a65"
       },
       "removedWidgets": [],
       "type": "mimeBundle"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "from sparkdl.xgboost import XgboostRegressor\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "import mlflow\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col, translate\n",
    "from pyspark.sql.functions import when\n",
    "import warnings\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from sparkdl.xgboost import XgboostRegressor\n",
    "from pyspark.sql.functions import log, col\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, Trials\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col, translate\n",
    "from pyspark.sql.functions import when\n",
    "import warnings\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from sparkdl.xgboost import XgboostRegressor\n",
    "from pyspark.sql.functions import log, col\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, Trials\n",
    "import numpy as np\n",
    "import mlflow.spark\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from databricks.feature_store import feature_table, FeatureLookup\n",
    "from pyspark.sql.functions import monotonically_increasing_id, lit, expr, rand\n",
    "import uuid\n",
    "from databricks import feature_store\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from mlflow.tracking import MlflowClient\n",
    "from databricks.feature_store import feature_table, FeatureLookup\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d375847a-1837-463f-bea4-55cea7a729bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def data_reading_table(table_name):\n",
    "    raw_df=spark.sql(f'''select * from {table_name}''')\n",
    "    return raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9ed55a0-af72-4c27-8078-294d9e194de3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def data_reading_table_select_col(table_name,select_col):\n",
    "    raw_df=spark.sql(f'''select {select_col} from {table_name}'''.replace('\"','').replace(\"'\",''))\n",
    "    return raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6844ee17-411f-4364-8b60-e423233a6125",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_run_details(run):\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    client = MlflowClient()\n",
    "    experiment_id = run.info.experiment_id\n",
    "    run_id = run.info.run_id\n",
    "    runs_df = mlflow.search_runs(experiment_id, order_by=[\"attributes.start_time desc\"], max_results=2)\n",
    "    print(runs_df.display())\n",
    "    return experiment_id,run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d837858-869a-4587-8480-1f16aca39fbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_best_model(run):\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    client = MlflowClient()\n",
    "    experiment_id = run.info.experiment_id\n",
    "    run_id = run.info.run_id\n",
    "    runs_df = mlflow.search_runs(experiment_id, order_by=[\"attributes.start_time desc\"], max_results=2)\n",
    "    print(runs_df.display())\n",
    "    return experiment_id,run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "544e1aa6-a72e-4f65-b062-0973dd94f1f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def data_pipeline(df,x_col,y_col):\n",
    "    categorical_cols = [field for (field, dataType) in df.dtypes if dataType == \"string\"]\n",
    "    index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "    ohe_output_cols = [x + \"OHE\" for x in categorical_cols]\n",
    "    \n",
    "    string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
    "    ohe_encoder = OneHotEncoder(inputCols=index_output_cols, outputCols=ohe_output_cols)\n",
    "    numeric_cols = [field for (field, dataType) in df.dtypes if ((dataType == \"double\") & (field !=y_col))]\n",
    "    assembler_inputs = ohe_output_cols + numeric_cols\n",
    "    vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=x_col)\n",
    "    \n",
    "    return string_indexer,ohe_encoder,vec_assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a05f4e-dbae-4e38-aafe-6bb7d0882e7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rf_model(params,x_col,y_col):\n",
    "    max_depth=params[0]\n",
    "    num_trees=params[1]\n",
    "    max_bins=params[2]\n",
    "    rf_params = {\n",
    "            \"maxDepth\": max_depth,\n",
    "            \"numTrees\": num_trees,\n",
    "            \"maxBins\": max_bins\n",
    "        }\n",
    "        # Define model\n",
    "    rf = RandomForestRegressor(featuresCol=x_col,labelCol=y_col, **rf_params)\n",
    "    rf.setSeed(5678)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "698676d4-28ad-43fd-92f2-2fadae9dcbb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def lr_model(regParam,x_col,y_col):\n",
    "    lr_params = {\n",
    "            \"regParam\": regParam\n",
    "        }\n",
    "        # Define model\n",
    "    lr = LinearRegression(featuresCol=x_col,labelCol=y_col, **lr_params)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "338f5b96-615b-42f7-98ad-44c229f8085b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def xgb_model(params,x_col,y_col):\n",
    "    n_estimators=params[0]\n",
    "    learning_rate=params[1]\n",
    "    max_depth=params[2]\n",
    "    xgb_params = {\n",
    "         \"n_estimators\": n_estimators, \n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"max_depth\": max_depth,\n",
    "        \"missing\": 0,\n",
    "        \"nthread\":4,\n",
    "        \"seed\":5678\n",
    "        }\n",
    "        # Define model\n",
    "    xgb = XgboostRegressor(featuresCol=x_col,labelCol=y_col,**xgb_params)\n",
    "    return xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e39bbc6-902c-418d-bf81-0914d681ab5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def data_split(data):\n",
    "    train_df, test_df = (data.repartition(24).randomSplit([0.8, 0.2], seed=42))\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92959772-85c2-4425-970f-b1bdee5a88ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_model(model_type,parma,train_df,test_df,model,x_col,y_col):\n",
    "        string_indexer,ohe_encoder,vec_assembler=data_pipeline(train_df,x_col,y_col)\n",
    "        #Pipeline\n",
    "        pipeline = Pipeline(stages=[string_indexer, ohe_encoder, vec_assembler, model])\n",
    "        # Fit the model\n",
    "        pipeline_model=pipeline.fit(train_df)\n",
    "        # make prediction \n",
    "        pred_df = pipeline_model.transform(test_df)\n",
    "         # Evaluate and log metrics       \n",
    "        regression_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=y_col)\n",
    "        r2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
    "        rmse = regression_evaluator.setMetricName(\"rmse\").evaluate(pred_df)\n",
    "        print(f\"RMSE is {rmse}\")\n",
    "        print(f\"R2 is {r2}\")\n",
    "        \n",
    "        mlflow_log(model_type,parma,x_col,y_col,pipeline_model,rmse,r2)\n",
    "        return train_df,test_df, model, pipeline,regression_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96e8012-4a15-4730-9db1-f924540b2d5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model_and_predict(run_id,test_df):\n",
    "    model_path = f\"runs:/{run_id}/model\"\n",
    "    loaded_model = mlflow.spark.load_model(model_path)\n",
    "    pred_df=loaded_model.transform(test_df)\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb23b2c4-1736-4e35-ad2d-34cc6118d07f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mlflow_log(model_type,parma,x_col,y_col,pipeline_model,rmse,r2):\n",
    "#         mlflow.log_param(\"label\", x_col)\n",
    "#         mlflow.log_param(\"features\", y_col)\n",
    "        mlflow.log_param(\"paramater\",parma)\n",
    "        mlflow.spark.log_model(pipeline_model, \"model\") \n",
    "        mlflow.log_metric(\"RMSE\", rmse)\n",
    "        mlflow.log_metric(\"R2\", r2)\n",
    "        if model_type==\"Random Forest\":\n",
    "            mlflow.log_param(\"model type\", model_type)\n",
    "        elif model_type==\"Linear Regression\":\n",
    "            mlflow.log_param(\"model type\", model_type)\n",
    "        elif model_type==\"XGB\":\n",
    "            mlflow.log_param(\"model type\", model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fcd9e8d-4a6f-46f7-a566-879e8f749cf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def objective_function_rf(params):    \n",
    "\n",
    "    # set the hyperparameters that we want to tune\n",
    "    max_depth = params[\"max_depth\"]\n",
    "    num_trees = params[\"num_trees\"]\n",
    "    max_bins = params[\"max_bins\"]\n",
    "\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=\"tuning parmater for RF\"):\n",
    "        estimator = pipeline.copy({rf.maxDepth: max_depth, rf.numTrees: num_trees,rf.maxBins:max_bins})\n",
    "        model = estimator.fit(train_df_t)\n",
    "        preds = model.transform(test_df_t)\n",
    "        rmse = regression_evaluator.evaluate(preds)\n",
    "        mlflow.log_metric(\"RMSE\", rmse)\n",
    "        mlflow.log_param(\"paramater\",params)\n",
    "        mlflow.log_param(\"model type\", \"Random Forest\")\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04dec501-3229-4c36-815d-ad533cb78141",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def objective_function_lr(params):    \n",
    "   \n",
    "    # set the hyperparameters that we want to tune\n",
    "    regParam = params[\"regParam\"]\n",
    "\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=\"tuning parmater for LR\"):\n",
    "        estimator = pipeline.copy({lr.regParam: regParam})\n",
    "        model = estimator.fit(train_df_t)\n",
    "        preds = model.transform(test_df_t)\n",
    "        rmse = regression_evaluator.evaluate(preds)\n",
    "        mlflow.log_metric(\"RMSE\", rmse)\n",
    "        mlflow.log_param(\"paramater\",params)\n",
    "        mlflow.log_param(\"model type\", \"Linear Regression\")\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461d6dbb-0466-4bd7-bf9f-a8492eb8c0fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def objective_function_xgb(params):    \n",
    "    \n",
    "    # set the hyperparameters that we want to tune\n",
    "    n_estimators = int(params[\"n_estimators\"])\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    max_depth = int(params[\"max_depth\"])\n",
    "\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=\"tuning parmater for XGB\"):\n",
    "        estimator = pipeline.copy({xgb.n_estimators: n_estimators, xgb.learning_rate: learning_rate,xgb.max_depth:max_depth})\n",
    "        model = estimator.fit(train_df_t)\n",
    "        preds = model.transform(test_df_t)\n",
    "        rmse = regression_evaluator.evaluate(preds)\n",
    "        mlflow.log_metric(\"RMSE\", rmse)\n",
    "        mlflow.log_param(\"paramater\",params)\n",
    "        mlflow.log_param(\"model type\", \"XGB\")\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83503db3-1845-463e-91ad-bd5f88f07b47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train(model_type,train_df,test_df,x_col,y_col,params,experiment_id):\n",
    "    print('train baseline model>>>>>>>>>>>>>>')\n",
    "    \n",
    "    if model_type ==\"Linear Regression\":\n",
    "        model=lr_model(params,x_col,y_col)\n",
    "        params_logs = {\"regParam\":params}\n",
    "    elif model_type ==\"Random Forest\":\n",
    "        model=rf_model(params,x_col,y_col)\n",
    "        params_logs = {\"max_depth\":params[0],\"num_trees\":params[1],\"max_bins\":params[2]}\n",
    "    elif model_type ==\"XGB\":\n",
    "        model=xgb_model(params,x_col,y_col)\n",
    "        params_logs = {\"n_estimators\":params[0],\"learning_rate\":params[1],\"max_depth\":params[2]}\n",
    "\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=f'baseline model accuracy of {model_type}') as run:\n",
    "        train_df, test_df, model, pipeline , regression_evaluator = run_model(model_type,params_logs,train_df,test_df,model,x_col,y_col)\n",
    "    \n",
    "    experiment_id,run_id = get_run_details(run)\n",
    "    print(\"experiment_id :\",experiment_id)\n",
    "    print(\"run_id :\",run_id)\n",
    "    mlflow.end_run()\n",
    "\n",
    "    ##predict with load model\n",
    "    pred_df=load_model_and_predict(run_id,test_df)\n",
    "    return train_df, test_df, model, pipeline , regression_evaluator,experiment_id,run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7995fb-0c72-48ed-965d-2ee32bdfeac5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tune(model_type,train_df,test_df,x_col,y_col,params):\n",
    "    print (\"tuning paramater in search space\")\n",
    "    if model_type ==\"Linear Regression\":\n",
    "        objective_function=objective_function_lr\n",
    "    elif model_type ==\"Random Forest\":\n",
    "        objective_function=objective_function_rf\n",
    "    elif model_type ==\"XGB\":\n",
    "        objective_function=objective_function_xgb\n",
    "\n",
    "    print (\"tuning paramater in search space\")\n",
    "    print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "    num_evals = 5\n",
    "    trials = Trials()\n",
    "    best_hyperparam = fmin(fn=objective_function, \n",
    "                           space=search_space,\n",
    "                           algo=tpe.suggest, \n",
    "                           max_evals=num_evals,\n",
    "                           trials=trials,\n",
    "                           rstate=np.random.default_rng(42))\n",
    "    print (\"tuning paramater end\")\n",
    "    print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "    return best_hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f900407-4324-4540-82b2-44f39c1286d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import mlflow\n",
    "\n",
    "# # Set the tracking URI to Databricks\n",
    "# mlflow.set_tracking_uri(\"databricks\")\n",
    "\n",
    "# # Set the name of the experiment\n",
    "# experiment_name = \"/Users/kexin@mz.co.kr/demo_mlflow\"\n",
    "\n",
    "# # Create the experiment\n",
    "# mlflow.create_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc824bb-2c1a-45c0-a7ee-99ff678ceb8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_id='4294012359381903'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37aa3686-7109-43c4-b44c-d74ed56ddab8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_data(feature_table_name,dlt_table_name, feature_list ,lookup_key,y_col):\n",
    "    raw_df=spark.sql(f'''select * from {dlt_table_name}''')\n",
    "    ##inference random number column 추가: create_training_set 할때 inference_data_df 필수 파라미터고 왜 필수적인건지 알고 싶습니다.\n",
    "    ##현재 inference은 feature store에 데이터 읽은 후에 추가하는 방식으로 되어 있는데 혹시 inference 컬럼은 gold table 만들때 추가 해도 되는지 알고 싶습니다.\n",
    "    inference_data_df=raw_df.select(lookup_key, y_col,(rand() * 0.5-0.25).alias(\"rand_num\"))\n",
    "    \n",
    "    fs=FeatureStoreClient()\n",
    "    model_feature_lookups=[FeatureLookup(table_name=feature_table_name, feature_names =feature_list ,lookup_key=lookup_key)]\n",
    "    df=fs.create_training_set(inference_data_df, model_feature_lookups, label=y_col, exclude_columns=lookup_key).load_df()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d96d2697-715f-41de-b2f5-d5ab452e281b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def register_baseline_model(model_name,run_id):\n",
    "    model_uri = f\"runs:/{run_id}/model\"\n",
    "    model_details = mlflow.register_model(model_uri=model_uri, name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f0f676-f344-4fc9-a667-5fdc04ab252d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# create or replace table airbnb_db.feature_store_tb(feature_id string,\n",
    "# created_timestamp timestamp\n",
    "# ) "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "0",
   "notebookOrigID": 1264345210877595,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
